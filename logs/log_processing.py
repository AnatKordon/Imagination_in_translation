""" _summary_

    this file takes all the data generated by the app and analyzes it. 
    The analysis include:
    plotting the cosine distance , similiarity_score, subjective similarity score per uid
    doing pearson correlation between the cosine distance and the subjective similarity score per user
"""

import pandas as pd
import os
import glob
from tabulate import tabulate
from scipy.stats import pearsonr
import sys
import matplotlib.pyplot as plt

def normalize_score_to_cosine_distance_scale(
    score: float,
    old_max: float = 6.0,
    old_min: float = 1.0,
    new_min: float = 0.0,
    new_max: float = 2.0
) -> float:
    """
    Normalizes a score (e.g.: range [1, 6]) to the numerical range of cosine distance (e,g: range [0, 2]).

    This function transforms your original score so it becomes directly comparable
    to a cosine distance value.

    Args:
        score (float): The score from 1 to 6.
        old_min, old_max - old range
        new_max, new_min - new range
    Returns:
        float: The input score, normalized to the [0, 2] range, representing
               its equivalent value on the cosine distance scale.
               A score of 1 maps to 0 (most similar), and a score of 6 maps to 2 (most dissimilar).
    """
    
    # Ensure the input is within the expected range, or clamp it
    # This prevents unexpected outputs if the score is outside range
    if not (old_min <= score <= old_max):
        # In a real application, you might raise an error or handle this differently.
        # For demonstration, a warning and clamping is sufficient.
        print(f"Warning: Input score {score} is outside the expected range [{old_min}, {old_max}]. Clamping value.")
        score = max(old_min, min(score, old_max))

    # Step 1: Normalize the score to a [0, 1] range
    # This expresses the score's position proportionally within its original range
    normalized_to_0_1 = (score - old_min) / (old_max - old_min)

    # Step 2: Scale this [0, 1] normalized score to the new range
    # Then shift it to start at the new_min (which is 0 in this case)
    normalized_score_cd_scale = (normalized_to_0_1 * (new_max - new_min)) + new_min

    return normalized_score_cd_scale




def read_all_csv_files(directory_path="."):
    """
    Read all CSV files from a directory and combine them into a single DataFrame
    
    Parameters:
    - directory_path: Path to directory containing CSV files (default is current directory)
    
    Returns:
    - Combined pandas DataFrame
    """
    # Check if the directory exists before proceeding
    if not os.path.isdir(directory_path):
        raise FileNotFoundError(f"Directory does not exist: {directory_path}")
    
    csv_pattern = os.path.join(directory_path, "*.csv")
    csv_files = glob.glob(csv_pattern)
    if not csv_files:
        raise FileNotFoundError(f"No CSV files found in directory: {directory_path}")    
    
    # Get all CSV files in the directory
    csv_pattern = os.path.join(directory_path, "*.csv")
    print(csv_pattern)
    csv_files = glob.glob(csv_pattern)
           
    
    # Read and combine all CSV files
    dataframes = []
    for file in csv_files:
        try:
            df = pd.read_csv(file)
            # Add a column to track which file the data came from
            df['source_file'] = os.path.basename(file)
            dataframes.append(df)
            print(f"Successfully read: {file}")
        except Exception as e:
            print(f"Error reading {file}: {e}")
    
    if dataframes:
        # Combine all DataFrames
        combined_df = pd.concat(dataframes, ignore_index=True)
        print(f"\nCombined {len(dataframes)} CSV files into DataFrame with {len(combined_df)} rows")
        return combined_df
    else:
        return pd.DataFrame()

# Read all CSV files from current directory (logs folder)
df = read_all_csv_files(r'.\logs\user_data\\')

if df.empty:
    assert not df.empty, "No data found. Exiting script."

avg_similarity = df.groupby(['uid', 'session'])['similarity'].mean()
avg_cosine_distance = df.groupby(['uid', 'session'])['cosine_distance'].mean()
avg_subjective_score = df.groupby(['uid', 'session'])['subjective_score'].mean()
# Normalize avg_similarity using the provided normalization function
avg_similarity_normalized = avg_similarity.apply(
    lambda x: normalize_score_to_cosine_distance_scale(x, old_min=0, old_max=100)
)
avg_subjective_score_normalized = avg_subjective_score.apply(normalize_score_to_cosine_distance_scale)

# Combine all averages into a single DataFrame
avg_df = pd.DataFrame({
    'avg_similarity': avg_similarity,
    'avg_cosine_distance': avg_cosine_distance,
    'avg_subjective_score': avg_subjective_score,
    'avg_similarity_normalized': avg_similarity_normalized,
    'avg_subjective_score_normalized' : avg_subjective_score_normalized
}).reset_index()

print("\nAverage similarity, cosine distance, and subjective score per uid per session:")
# Print a more compact table (rounded values, fewer decimals, smaller format)
compact_df = avg_df.copy()
for col in ['avg_similarity', 'avg_cosine_distance', 'avg_subjective_score', 'avg_similarity_normalized', 'avg_subjective_score_normalized']:
    compact_df[col] = compact_df[col].round(3)
print(tabulate(compact_df, headers='keys', tablefmt='simple', showindex=False, numalign="right"))

# Create line graph for each uid
plt.figure(figsize=(12, 8))
    
# Get unique UIDs
unique_uids = df['uid'].unique()

for uid in unique_uids:
    # Get data for this specific uid from avg_df
    uid_df = avg_df[avg_df['uid'] == uid]
    sessions = uid_df['session']
    # Plot avg_similarity_normalized
    plt.plot(sessions, uid_df['avg_similarity_normalized'], marker='s', linestyle='--', label=f'UID: {uid} - Similarity (Norm)', linewidth=2)
    # Plot avg_cosine_distance
    plt.plot(sessions, uid_df['avg_cosine_distance'], marker='^', linestyle='-.', label=f'UID: {uid} - Cosine Dist', linewidth=2)
    # Plot avg_subjective_score_normalized
    plt.plot(sessions, uid_df['avg_subjective_score_normalized'], marker='x', linestyle=':', label=f'UID: {uid} - Subjective (Norm)', linewidth=2)

  
# Add plot formatting AFTER the loop
plt.xlabel('Session Number')
plt.ylabel('Average Score')
plt.title('Average Score Across Sessions by UID')
plt.legend()
plt.grid(True, alpha=0.3)
#plt.show()

print("\nPearson correlation between avg_cosine_distance and avg_similarity per uid:")
for uid in avg_df['uid'].unique():
    uid_data = avg_df[avg_df['uid'] == uid]
    if len(uid_data) > 1:
        corr, pval = pearsonr(uid_data['avg_cosine_distance'], uid_data['avg_similarity'])
        print(f"UID: {uid} | Pearson r: {corr:.3f} | p-value: {pval:.3g}")
    else:
        print(f"UID: {uid} | Not enough data for correlation (need at least 2 sessions)")